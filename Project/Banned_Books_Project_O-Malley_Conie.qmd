---
title: "Final Project"
author: "Conie O'Malley"
date: "`r Sys.Date()`"
format: 
  pdf: 
    titlepage: true
    toc: true
    toc-depth: 2
    code-block-wrap: true
    number-sections: true
execute:
  echo: true
  freeze: false
  error: false
  jupyter: python3
python:
  version: /Users/coniecakes/anaconda3/envs/datascience/bin/python
---

```{r library chunk}
required_packages <- c("jsonlite", "httr", "stringdist", "quanteda", "gutenbergr", "tm", "stringr", "textclean",
                        "tokenizers", "tidytext", "wordcloud", "RColorBrewer", "ggplot2", "ggthemes", "ggraph")

# Check if all required packages are installed and install them if not
for (pkg in required_packages) {
    if (!requireNamespace(pkg, quietly = TRUE)) {
        renv::install(pkg)
    }
        library(pkg, character.only = TRUE)
}
data("gutenberg_metadata", package = "gutenbergr")
```

```{r data import}
banned_books_file_path <- "/Users/coniecakes/Library/CloudStorage/OneDrive-Personal/001. Documents - Main/023. Programming Tools/R Studio/AdvTxtAnalytics/Project/data/PEN America's Index of School Book Bans (July 1, 2023 - June 30, 2024).xlsx" # assign file path to variable

banned_books_list <- readxl::read_excel(banned_books_file_path, sheet = "Sorted by Author & Title", skip = 2) # import banned books list

utils::head(banned_books_list, 10) # sanity check to make sure data was read in properly
```

```{r data preparation}
colnames(banned_books_list) <- c("Title", "Author", "Secondary_Author", "Illustrator", "Translator", 
                                "Series_Name", "State", "District", "Date_Removed", "Ban_Status", "Initiating_Action") # assign new column titles

banned_books_cleaned <- banned_books_list %>% 
    dplyr::select(Title, Author, State, District, Date_Removed, Ban_Status, Initiating_Action) %>% # select only relevant columns
    dplyr::mutate(Date_Removed = as.Date(lubridate::my(Date_Removed))) # convert date column to Date class

head(banned_books_cleaned,10) # sanity check

banned_books_cleaned %>% 
    dplyr::summarise(dplyr::across(dplyr::everything(), ~ sum(is.na(.)), .names = "NA_count_{.col}")) # check for NA values in the date column

fl_banned_books_list <- banned_books_cleaned %>% 
    dplyr::filter(State == "Florida") %>% 
    dplyr::distinct(Title, .keep_all = TRUE)

md_banned_books_list <- banned_books_cleaned %>% 
    dplyr::filter(State == "Maryland") %>% 
    dplyr::distinct(Title, .keep_all = TRUE)

ia_banned_books_list <- banned_books_cleaned %>% 
    dplyr::filter(State == "Iowa") %>% 
    dplyr::distinct(Title, .keep_all = TRUE)
```

## Data Sampling

I sampled 10 book titles from the state of Florida's banned books list. I originally wanted to use New York for comparison, but they only had 6 titles, so I selected Maryland as my subgroup comparison.

```{r data sampling}
seed <- 245 # select random seed
set.seed(seed) # set seed for reproducibility

# sample books from Florida & New York
fl_books_sample <- fl_banned_books_list %>% 
    dplyr::sample_n(10) 

md_books_sample <- md_banned_books_list %>% 
    dplyr::sample_n(10)

# print sampled books
print(fl_books_sample)
print(md_books_sample)
```

## Data Collection

### Gutenberg Search

I began to search for data online. I cleaned the book titles from the Florida sample and cleaned the titles from the Gutenberg library (`gutenbergr`) to ensure they matched. I then compared the two lists and found that none of the Florida titles were also in the Gutenberg library. This caused me to pivot to search for any ISBNs from the Florida banned books list sample to attempt to search them in other libraries.

```{r data collection}
# search for titles in gutenberg library
fl_sample_books_titles_cleaned <- fl_books_sample %>% 
    dplyr::select(Title) %>% 
    dplyr::pull() %>% 
    tolower() %>% 
    stringr::str_trim()

gutenberg_titles_cleaned <- gutenbergr::gutenberg_works() %>% 
    dplyr::select(title) %>% 
    dplyr::pull() %>% 
    tolower() %>% 
    stringr::str_trim()

in_gutenberg <- c()

for(i in fl_sample_books_titles_cleaned) {
  if (i %in% gutenberg_titles_cleaned) {
    in_gutenberg <- c(in_gutenberg, i)
  }
}

in_gutenberg
```

### ISBN Search

I could not obtain any ISBNs from the below function I wrote. I spent a few hours on this, then decided to move on to another method of searching.

```{r isbn search function}
# create isbn search function
get_isbn_google <- function(title) {
    search_url <- paste0("https://www.googleapis.com/books/v1/volumes?q=intitle:", gsub(" ", "+", title))
    response <- httr::GET(search_url)
    if (httr::status_code(response) != 200){
        return("Search unsuccessful")
    }
    book_data <- tryCatch({
        httr::content(response, "text", encoding = "UTF-8") %>% jsonlite::fromJSON()
    }, error = function(e) return(NA))
    if (!"items" %in% names(book_data) || length(book_data$items) == 0) {
        return("No Results Found")
    }
    if (!"items" %in% names(book_data) || !is.list(book_data$items) || length(book_data$items) == 0) {
        return("No Item Exists")
    }
    first_item <- book_data$items[[1]]

    if (!"volumeInfo" %in% names(first_item) || !is.list(first_item$volumeInfo)) {
        return("No Volume Exists")
    }
    volume_info <- first_item$volumeInfo
    if (!"industryIdentifiers" %in% names(volume_info) || !is.list(volume_info$industryIdentifiers)) {
        return("No ISBN Available")  # No ISBN available
    }
    identifiers <- volume_info$industryIdentifiers

    isbn <- NA
    for (id in identifiers) {
        if (id$type == "ISBN_13") {
            isbn <- id$identifier
            break
        } else if (id$type == "ISBN_10") {
            isbn <- id$identifier
        }
    }
    return(isbn)
}

# function test
isbn_example <- get_isbn_google("Slaughterhouse-Five")
print(isbn_example)
```

### Gutenberg Search - Pt. 2

I cleaned all book titles from the Florida banned books list and compared them to the Gutenberg library. My goal is to find a suitable sample of texts here that I can use before reverting to my original plan of using themes and descriptions from the Google Books API.

```{r full gutenberg search}
# create a function to search for titles in a library
full_gutenberg_search <- function(banned_books, gutenberg_titles) {
    matched_books <- banned_books[banned_books %in% gutenberg_titles]
    return(matched_books)
}
```

I have identified titles that I can pull from the Gutenberg library for my project. I am going to take a sample of 10 texts from each, Florida and Iowa, and use them as my sample texts. I ended up getting matches for 16 books from the Iowa banned books list and 53 from Florida. After matching the titles, I will see how many I can downloand and I may need to revise my sample numbers.

```{r full title search}
fl_books_titles_cleaned <- fl_banned_books_list %>% # cleaned fl titles list
    dplyr::select(Title) %>% 
    dplyr::pull() %>% 
    tolower() %>% 
    stringr::str_trim()

md_books_titles_cleaned <- md_banned_books_list %>% # cleaned md titles list
    dplyr::select(Title) %>% 
    dplyr::pull() %>% 
    tolower() %>% 
    stringr::str_trim()

ia_books_titles_cleaned <- ia_banned_books_list %>% # cleaned ia titles list
    dplyr::select(Title) %>% 
    dplyr::pull() %>% 
    tolower() %>% 
    stringr::str_trim()

fl_match_list <- full_gutenberg_search(fl_books_titles_cleaned, gutenberg_titles_cleaned) # fl match list
fl_match_list

md_match_list <- full_gutenberg_search(md_books_titles_cleaned, gutenberg_titles_cleaned) # md match list
md_match_list

ia_match_list <- full_gutenberg_search(ia_books_titles_cleaned, gutenberg_titles_cleaned) # ia match list
ia_match_list
```

Here I created a fuzzy title search function to allow proper title matching and minimizing the effects of grammar differences.

```{r title search function}
fuzzy_title_search <- function(book_list) {
    gutenberg_metadata <- gutenbergr::gutenberg_works()
    matched_books <- book_list %>% 
        sapply(function(book) {
            distances <- stringdist::stringdist(book, gutenberg_metadata$title, method = "jw")
            closest_match <- gutenberg_metadata$title[which.min(distances)]
            return(closest_match)   
        })
        return(matched_books)
}
```

Here I actually pull the sample of my data that I will use in the following steps.

```{r data re-sampling}
ia_sample <- sample(ia_match_list, 10)
fl_sample <- sample(fl_match_list, 10)

cat("IA Sample:\n",{ia_sample}, sep="\n")
cat("\nFL Sample:\n",{fl_sample}, sep="\n")
```


Here I ran my fuzzy matching title search. I was able to match all but one of the sampled titles, which I've identified and will add in by hand later on.

```{r fuzzy matching}
# Iowa fuzzy matching
ia_fuzzy_matches <- fuzzy_title_search(ia_sample) # incorrectly selected 1 title
ia_fuzzy_matches

ia_fuzzy_matches_full <- fuzzy_title_search(ia_match_list)

# Florida fuzzy matching
fl_fuzzy_matches <- fuzzy_title_search(fl_sample)
fl_fuzzy_matches

fl_fuzzy_matches_full <- fuzzy_title_search(fl_match_list)
```

I ran into a lot of problems downloading books. After my first pass, I was only able to get 6 / 20, and on my second pass I was only able to get 10/26. I am going to try and download directly from gutenberg's website, but I may need to adjust my sample sizes for subgroup comparison.

```{r download texts}
# retrieve Iowa text ids
ia_gutenberg_ids <- gutenbergr::gutenberg_works() %>%
  dplyr::filter(title %in% ia_fuzzy_matches) %>%
  dplyr::select(gutenberg_id, title)
# correct "Dead End" entry
correct_entry <- gutenbergr::gutenberg_works() %>% 
    dplyr::filter(tolower(title) == "dead end") %>% 
    dplyr::select(gutenberg_id, title)
correct_entry
ia_gutenberg_ids$gutenberg_id[10] <- correct_entry$gutenberg_id
ia_gutenberg_ids$title[10] <- correct_entry$title

ia_gutenberg_ids_full <- gutenbergr::gutenberg_works() %>%
  dplyr::filter(title %in% ia_fuzzy_matches_full) %>%
  dplyr::select(gutenberg_id, title)

# retrieve Florida text ids
fl_gutenberg_ids <- gutenbergr::gutenberg_works() %>%
  dplyr::filter(title %in% fl_fuzzy_matches) %>%
  dplyr::select(gutenberg_id, title)

fl_gutenberg_ids_full <- gutenbergr::gutenberg_works() %>%
  dplyr::filter(title %in% fl_fuzzy_matches_full) %>%
  dplyr::select(gutenberg_id, title)
```

Now I am attempting to download the balance of books from the gutenberg website directly.

```{r gutenberg web search function}
load_gutenberg_text <- function(book_id) {
    url <- paste0("https://www.gutenberg.org/files/", book_id, "/", book_id, "-0.txt")
    response <- httr::GET(url)
    if (status_code(response) == 200) {
        text <- content(response, "text", encoding = "UTF-8")
        return(text)
    } else {
        message(paste("Book ID", book_id, "could not be loaded."))
        return(NULL)
    }
}
```

I was able to pull in all of the texts necessary for the Florida banned books list - both through dowload from `gutenbergr` and the Project Gutenberg website. I've assigned it to a data frame `fl_book_texts_df`. 

```{r Florida texts}
# download Florida texts and set up corpus
fl_book_texts <- gutenbergr::gutenberg_download(fl_gutenberg_ids$gutenberg_id)
fl_book_texts <- fl_book_texts %>% 
    dplyr::group_by(gutenberg_id) %>% 
    dplyr::summarise(text = paste0(text, collapse = " "))
# identify missing texts
fl_missing_books <- fl_gutenberg_ids %>%
  dplyr::filter(!(gutenberg_id %in% fl_book_texts$gutenberg_id))

fl_missing_book_texts_list <- list()
for (i in seq_along(fl_missing_books$gutenberg_id)) {
    book_id <- fl_missing_books$gutenberg_id[i]
    book_title <- fl_missing_books$title[i]
    book_text <- load_gutenberg_text(book_id)
    if (!is.null(book_text)) {
        fl_missing_book_texts_list[[as.character(book_id)]] <- data.frame(
            gutenberg_id = book_id,
            text = book_text,
            stringsAsFactors = FALSE
        )
    }
}
fl_missing_book_texts_df <- dplyr::bind_rows(fl_missing_book_texts_list)
fl_book_texts_df <- dplyr::bind_rows(fl_book_texts, fl_missing_book_texts_df) # florida books data frame
#utils::write.csv(fl_book_texts_df, "Project/data/fl_book_texts.csv") # write to csv so I don't have to do this again
```

I pulled the book title, id, and text for the Iowa sample and added in the missing book title from the fuzzy matching. I had a number of issues downloading book texts so I had to do some exta work obtaining the data. Ultimately, I ended up with `ia_book_texts_df`.

```{r iowa books data download}
ia_books_text_list <- list()
for (i in seq_along(ia_gutenberg_ids$gutenberg_id)) {
    book_id <- ia_gutenberg_ids$gutenberg_id[i]
    book_title <- ia_gutenberg_ids$title[i]
    book_text <- load_gutenberg_text(book_id)
    if (!is.null(book_text)) {
        ia_books_text_list[[as.character(book_id)]] <- data.frame(
            gutenberg_id = book_id,
            title = book_title,
            text = book_text,
            stringsAsFactors = FALSE
        )
    }
}

ia_book_texts_df <- dplyr::bind_rows(ia_books_text_list)

# replace 1 book that could not be downloaded
missing_books <- ia_gutenberg_ids_full %>%
  dplyr::filter(!(title %in% ia_book_texts_df$title))

if (nrow(missing_books) > 0) {
    ia_additional_sample <- missing_books %>%
        dplyr::slice_sample(n = 1)  
    print(ia_additional_sample) 
} else {
    print("No additional books available for sampling.")
}

ia_additional_sample_text <- load_gutenberg_text(ia_additional_sample$gutenberg_id)
ia_additional_sample_text_df <- data.frame(gutenberg_id = book_id,
            title = book_title,
            text = book_text,
            stringsAsFactors = FALSE)
ia_book_texts_df <- dplyr::bind_rows(ia_book_texts_df, ia_additional_sample_text_df) 

dreams_end <- data.frame(gutenberg_id = as.integer(68179), text = load_gutenberg_text(68170), stringsAsFactors = FALSE)

ia_book_texts_df <- dplyr::bind_rows(ia_book_texts_df, dreams_end)

ia_book_texts_df <- ia_book_texts_df %>% 
    dplyr::select(-title) %>%
    dplyr::bind_rows(dreams_end) # Iowa banned books list

# had issues downloading books again - had to run a separate download to retrieve and reproduce the Iowa banned books list

redeemed <- gutenbergr::gutenberg_download(59277)
extra_book <- gutenbergr::gutenberg_download(16348)

redeemed <- redeemed %>% 
    dplyr::summarise(text = paste0(text, collapse = " ")) %>% 
    dplyr::mutate(gutenberg_id = as.integer(59277)) %>% 
    dplyr::select(gutenberg_id, text)

extra_book <- extra_book %>% 
    dplyr::summarise(text = paste0(text, collapse = " ")) %>% 
    dplyr::mutate(gutenberg_id = as.integer(16348)) %>% 
    dplyr::select(gutenberg_id, text)

ia_book_texts_df <- ia_book_texts_df %>% 
    dplyr::bind_rows(redeemed, extra_book)

# remove duplicates
ia_book_texts_df <- ia_book_texts_df[-c(9,10),]

ia_book_texts_df <- ia_book_texts_df %>%
  filter(gutenberg_id != 68179) # iowa banned books list

#utils::write.csv(ia_book_texts_df, "Project/data/ia_book_texts.csv") # write to csv so I don't have to do this again
```

### Title Additions

I needed to correct the title and author names for further processing. This is an ugly way to do it, but it was easier than trying to loop through the previous code to pull the data. On a larger data set, the time spent generating a loop or function to do this would be worth it, but it was not a beneficial use of time in this case. I added in the book titles and authors and rearragned the columns.

```{r title function}
fl_book_titles <- c("Wuthering Heights", "Leonardo Da Vinci", "The Pirate", "The Dark Tower", "Native Son", "Redeemed", "The Taming of the Shrew",
                    "The Road", "Chain Reaction", "The Heir")
fl_book_authors <- c("Emily Brontë", "Maurice W. Brockwell", "Captain Frederick Marryat", "Phyllis Bottome", "T. D. Hamm", "George Sheldon Downs",
                    "William Shakespeare", "Jack London", "Boyd Ellanby", "Sydney C. Grier")

ia_book_titles <- c("The Picture of Dorian Gray", "The Talisman", "Christine", "The Great Return", "Smoke", "Glass", "The Bridge", "Dead End", 
                    "Redeemed", "Dreamland")

ia_book_authors <- c("Oscar Wilde", "Sir Walter Scott", "Elizabeth Von Arnim", "Arthur Machen", "Ivan Sergeevich Turgenev", "Edward Dillon", 
                    "G. G. Revelle", "Wallace Macfarlane", "George Sheldon Downs", "Julie M. Lippmann")
ia_book_texts_df$title <- ia_book_titles
ia_book_texts_df$author <- ia_book_authors
fl_book_texts_df$title <- fl_book_titles
fl_book_texts_df$author <- fl_book_authors

fl_book_texts_df <- fl_book_texts_df %>% 
    dplyr::select(gutenberg_id, title, author, text)

ia_book_texts_df <- ia_book_texts_df %>% 
    dplyr::select(gutenberg_id, title, author, text)

```


### Google Books Metadata Collection

This was an interesting issue - I ran into troubles trying to write functions to return the metadata from google books, even with the API. I ended up using ChatGPT to return a csv of the titles, subjects, and descriptions, which I was able to read into a variable and left join to my texts data frames to have the complete data set required to begin data preprocessing. I now have my final data sets ready for preproccessing and analysis:

- `fl_book_texts_df`
- `ia_book_texts_df`

```{r metadata}
fl_book_metadata <- readr::read_csv("Project/data/fl_book_metadata.csv")
ia_book_metadata <- readr::read_csv("Project/data/ia_book_metadata.csv")

fl_book_texts_df %>% dplyr::left_join(fl_book_metadata, by = "title") -> fl_book_texts_df
ia_book_texts_df %>% dplyr::left_join(ia_book_metadata, by = "title") -> ia_book_texts_df
```

## Data Preprocessing

During the data preprocessing phase I will take all the text data and pass it through a function to:

- Assure UTF-8 enconding of text data
- Make all letters lowercase
- Remove Gutenberg boilerplate text
- Normalize quotation marks for accuracy
- Expand contractions
- Remove punctuation
- Remove special characters
- Remove extra white space and formatting
- Remove stopwords
- Remove numbers
- Tokenize sentences

### Custom Functions

Normally, I would structure this as a package and these functions would be in a separate directory where you could load them, but since I am submitting this as a stand alone qmd, I am including them in the body.

```{r preprocessing function}
clean_text <- function(text_vector) {
  # convert text to UTF-8 encoding
  text_vector <- iconv(text_vector, to = "UTF-8")

  # remove Project Gutenberg headers/footers
  text_vector <- gutenbergr::gutenberg_strip(text_vector)

  # chapter headers detection
  chapter_pattern <- "^(chapter [\\dIVXLC]|[\\dIVXLC]+\\.)"

  # trim between chapter headers
  chapter_indices <- grep(chapter_pattern, text_vector, ignore.case = TRUE)
  if (length(chapter_indices) > 0) {
    text_vector <- text_vector[chapter_indices[1]:chapter_indices[length(chapter_indices)]]
  }

  #convert all text to lowercase
  text_vector <- tolower(text_vector)

  # normalize quotes
  text_vector <- stringr::str_replace_all(text_vector, "[“”]", "\"")
  text_vector <- stringr::str_replace_all(text_vector, "[‘’]", "'")

  # expand contractions
  text_vector <- textclean::replace_contraction(text_vector)

  # remove punctuation
  text_vector <- stringr::str_remove_all(text_vector, "[[:punct:]]")

  # remove numbers
  text_vector <- stringr::str_remove_all(text_vector, "\\d+")

  # remove special characters and symbols
  text_vector <- stringr::str_replace_all(text_vector, "[^a-zA-Z\\s]", "")

  # remove extra whitespace and blank lines
  text_vector <- stringr::str_squish(text_vector)
  text_vector <- text_vector[text_vector != ""]

  # remove stopwords
  text_vector <- tm::removeWords(text_vector, tm::stopwords("en"))

  # remove ultra-short words (1-character)
  text_vector <- stringr::str_remove_all(text_vector, "\\b[a-zA-Z]\\b")

  # tokenize sentences
  text_vector <- tokenizers::tokenize_sentences(text_vector)

  # collapse list into a string
  text_vector <- sapply(text_vector, function(x) paste(x, collapse = " "))

  return(text_vector)
}
```

```{r text re-preprocessing}
clean_text_headers <- function(text) {
  # Split the text into lines and then combine into a single string
  text_vector <- unlist(strsplit(text, "\r\n|\n"))
  complete_text <- paste(text_vector, collapse = " ")
  
  # Define a regex pattern to capture the Gutenberg header
  header_start_pattern <- "(?i)\\*{3,}\\s*start of the project gutenberg ebook\\s*\\d+\\s*\\*{3,}"

  # Find the match for the header using str_locate to handle string matching
  header_match <- stringr::str_locate(complete_text, header_start_pattern)
  
  # If a match is found, remove that part of the text
  if (!is.na(header_match[1])) {
    # Calculate the position after the matched header
    header_end_pos <- header_match[2]
    # Remove text up to immediately after the header
    trimmed_text <- substring(complete_text, header_end_pos + 1)
  } else {
    # Return original text if no header is detected
    trimmed_text <- complete_text
  }

  return(trimmed_text)
}
```

```{r punctuation function}
# custom punctuation removal
remove_punctuation_custom <- function(text) {
  # remove punctuation including single and double quotes
  return(gsub("[[:punct:]\"']", "", text))
}
```

```{r theme detection function}
# theme detection function
detect_theme <- function(text, keywords) {
  pattern <- paste0("\\b(", paste(keywords, collapse = "|"), ")\\b")
  stringr::str_detect(tolower(text), pattern)
}
```

### Function Application 

```{r preprocess data}
# apply preprocessing function to each data set
fl_book_texts_df$cleaned_text <- sapply(fl_book_texts_df$text, clean_text)
ia_book_texts_df$cleaned_text <- sapply(ia_book_texts_df$text, clean_text)

fl_book_texts_df$cleaned_description <- sapply(fl_book_texts_df$description, clean_text)
ia_book_texts_df$cleaned_description <- sapply(ia_book_texts_df$description, clean_text)

fl_book_texts_df <- fl_book_texts_df %>% 
    dplyr::filter(!is.na(cleaned_description)) %>% 
    dplyr::filter(!is.na(cleaned_text))

ia_book_texts_df <- ia_book_texts_df %>% 
    dplyr::filter(!is.na(cleaned_description)) %>% 
    dplyr::filter(!is.na(cleaned_text))
```

```{r upload preprocessed data}
# removing additional stopwords from texts
enhanced_stopwords <- c(
  stopwords::stopwords("en", source = "stopwords-iso"),
  quanteda::stopwords("en"),
  tidytext::stop_words$word,
  "the", "and", "but"
) %>% unique()

# this is for working on data that has already been saved
fl_book_texts_df <- readr::read_csv("Project/data/fl_book_texts.csv")
fl_book_texts_df <- fl_book_texts_df %>% 
  dplyr::select(-...1) %>%
  dplyr::mutate(gutenberg_id = as.character(gutenberg_id)) %>% 
  dplyr::mutate(text = lapply(text, clean_text_headers)) %>% 
  dplyr::mutate(text = lapply(text, function(t) tm::removeWords(t, enhanced_stopwords))) %>% 
  dplyr::mutate(text = lapply(text, function(t) tm::removePunctuation(t))) %>% 
  dplyr::mutate(text = lapply(text, function(t) tm::removeNumbers(t))) %>% 
  dplyr::mutate(text = lapply(text, remove_punctuation_custom))
  

ia_book_texts_df <- readr::read_csv("Project/data/ia_book_texts.csv")
ia_book_texts_df <- ia_book_texts_df %>% 
  dplyr::select(-...1) %>%
  dplyr::mutate(gutenberg_id = as.character(gutenberg_id)) %>% 
  dplyr::mutate(text = lapply(text, clean_text_headers)) %>% 
  dplyr::mutate(text = lapply(text, function(t) tm::removeWords(t, enhanced_stopwords))) %>% 
  dplyr::mutate(text = lapply(text, function(t) tm::removePunctuation(t))) %>% 
  dplyr::mutate(text = lapply(text, function(t) tm::removeNumbers(t))) %>% 
  dplyr::mutate(text = lapply(text, remove_punctuation_custom))
```

## Data Analysis and Visualization 

```{r florida tfidf}
# calculate fl tf
fl_tf <- fl_book_texts_df %>%
  tidytext::unnest_tokens(word, text) %>%
  dplyr::filter(!word %in% enhanced_stopwords) %>%
  dplyr::count(gutenberg_id, word, sort = TRUE)

# calculate fl tfidf
fl_tfidf <- fl_tf %>%
  tidytext::bind_tf_idf(term = word, document = gutenberg_id, n = n) %>% 
  dplyr::left_join(
    fl_book_texts_df %>% dplyr::select(gutenberg_id, title),
    by = "gutenberg_id"
  )

# calculate top fl tfidf
top_fl_tfidf <- fl_tfidf %>%
  dplyr::group_by(gutenberg_id) %>%
  dplyr::arrange(dplyr::desc(tf_idf)) %>%
  dplyr::slice_head(n = 20)
```

```{r iowa tfidf}
# calculate ia tf
ia_tf <- ia_book_texts_df %>%
  tidytext::unnest_tokens(word, text) %>%
  dplyr::filter(!word %in% enhanced_stopwords) %>%
  dplyr::count(gutenberg_id, word, sort = TRUE)

# calculate ia tfidf
ia_tfidf <- ia_tf %>%
  tidytext::bind_tf_idf(term = word, document = gutenberg_id, n = n) %>% 
  dplyr::left_join(
    ia_book_texts_df %>% dplyr::select(gutenberg_id, title),
    by = "gutenberg_id"
  )

# calculate top ia tfidf
top_ia_tfidf <- ia_tfidf %>%
  dplyr::group_by(gutenberg_id) %>%
  dplyr::arrange(dplyr::desc(tf_idf)) %>%
  dplyr::slice_head(n = 20)
```


### Term Frequency Analysis {.tabset}

```{r florida tf viz}
# calculate top fl tf
top_fl_tf <- fl_tf %>% 
  dplyr::group_by(word) %>%
  dplyr::summarise(total = sum(n)) %>%
  dplyr::slice_max(order_by = total, n = 20)

# heatmap
fl_tf_heatmap <- fl_tfidf %>%
  dplyr::filter(word %in% top_fl_tf$word) %>%
  dplyr::select(gutenberg_id, title, word, n) %>% 
  tidyr::pivot_wider(names_from = word, values_from = n, values_fill = 0) %>%
  tidyr::pivot_longer(cols = -c(gutenberg_id, title), names_to = "word", values_to = "freq")

# Plot
ggplot2::ggplot(fl_tf_heatmap, ggplot2::aes(x = word, y = title, fill = freq)) +
  ggthemes::theme_wsj() +
  ggplot2::geom_tile() +
  ggplot2::scale_fill_viridis_c() +
  ggplot2::labs(title = "Florida: Heatmap of Top Term Frequencies",
                x = "Term", y = "Book Title") +
  ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 45, hjust = 1))
```

```{r ia tf viz}
# calculate top ia tf
top_ia_tf <- ia_tf %>% 
  dplyr::group_by(word) %>%
  dplyr::summarise(total = sum(n)) %>%
  dplyr::slice_max(order_by = total, n = 20)

# heatmap
ia_tf_heatmap <- ia_tfidf %>%
  dplyr::filter(word %in% top_ia_tf$word) %>%
  dplyr::select(gutenberg_id, title, word, n) %>% 
  tidyr::pivot_wider(names_from = word, values_from = n, values_fill = 0) %>%
  tidyr::pivot_longer(cols = -c(gutenberg_id, title), names_to = "word", values_to = "freq")

# plot
ggplot2::ggplot(ia_tf_heatmap, ggplot2::aes(x = word, y = title, fill = freq)) +
  ggthemes::theme_wsj() +
  ggplot2::geom_tile() +
  ggplot2::scale_fill_viridis_c() +
  ggplot2::labs(title = "Iowa: Heatmap of Top Term Frequencies",
                x = "Term", y = "Book Title") +
  ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 45, hjust = 1))
```


```{r fl wordcloud}
# build wordcloud
wordcloud::wordcloud(
  words = fl_tfidf$word,
  freq = fl_tfidf$n,
  min.freq = 5,
  max.words = 100,
  colors = RColorBrewer::brewer.pal(8, "Dark2"),
  random.order = FALSE
)
```


```{r iowa wordcloud}
wordcloud::wordcloud(
  words = ia_tfidf$word,
  freq = ia_tfidf$n,
  min.freq = 5,
  max.words = 100,
  colors = RColorBrewer::brewer.pal(8, "Dark2"),
  random.order = FALSE
)
```

```{r fl tfidf viz}
# calculate top tf-idf
fl_top_tfidf <- fl_tfidf %>%
  dplyr::group_by(gutenberg_id) %>%
  dplyr::slice_max(order_by = tf_idf, n = 10) %>%
  dplyr::ungroup()

# plot tf-idf
ggplot2::ggplot(fl_top_tfidf, ggplot2::aes(x = reorder(word, tf_idf), y = tf_idf)) +
  ggthemes::theme_wsj() +
  ggplot2::geom_col(fill = "steelblue") +
  ggplot2::facet_wrap(~ title, scales = "free_y") +
  ggplot2::coord_flip() +
  ggplot2::labs(title = "Top 10 TF-IDF Terms per Florida Document",
                x = "Term", y = "TF-IDF Score")
```


```{r iowa data visualization}
# calculate top ia tf
top_ia_tfidf <- ia_tfidf %>%
  dplyr::group_by(gutenberg_id) %>%
  dplyr::slice_max(order_by = tf_idf, n = 10) %>%
  dplyr::ungroup()

# Plot
ggplot2::ggplot(top_ia_tfidf, ggplot2::aes(x = reorder(word, tf_idf), y = tf_idf)) +
  ggthemes::theme_wsj() +
  ggplot2::geom_col(fill = "darkgreen") +
  ggplot2::facet_wrap(~ title, scales = "free_y") +
  ggplot2::coord_flip() +
  ggplot2::labs(
    title = "Top 10 TF-IDF Terms per Iowa Book",
    x = "Term", y = "TF-IDF Score"
  ) 
```

### Phrase Frequency Analysis

```{r fl phrase analysis}
# tokenize fl bigrams
fl_bigrams <- fl_book_texts_df %>%
  tidytext::unnest_tokens(bigram, text, token = "ngrams", n = 2)

# split and filter
fl_bigrams_filtered <- fl_bigrams %>%
  tidyr::separate(bigram, into = c("word1", "word2"), sep = " ") %>%
  dplyr::filter(!word1 %in% enhanced_stopwords, !word2 %in% enhanced_stopwords) %>%
  tidyr::unite(bigram, word1, word2, sep = " ")

# calculate fl tf
fl_bigram_tf <- fl_bigrams_filtered %>%
  dplyr::count(gutenberg_id, bigram, sort = TRUE)

# calculate fl tfidf
fl_bigram_tfidf <- fl_bigram_tf %>%
  tidytext::bind_tf_idf(term = bigram, document = gutenberg_id, n = n) %>%
  dplyr::left_join(
    fl_book_texts_df %>% dplyr::select(gutenberg_id, title),
    by = "gutenberg_id"
  )

# top bigrams per doc
top_fl_bigram_tfidf <- fl_bigram_tfidf %>%
  dplyr::group_by(gutenberg_id) %>%
  dplyr::slice_max(order_by = tf_idf, n = 10) %>%
  dplyr::ungroup()
```

```{r fl phrase analysis viz}
# split bigrams into related adjacent columns
fl_bigram_pairs <- fl_bigrams_filtered %>%
  tidyr::separate(bigram, into = c("word1", "word2"), sep = " ") %>%
  dplyr::count(word1, word2, sort = TRUE) %>%
  dplyr::filter(n >= 15)

fl_bigram_graph <- igraph::graph_from_data_frame(fl_bigram_pairs)

# plot bigram network
ggraph::ggraph(fl_bigram_graph, layout = "fr") +
  ggthemes::theme_wsj() +
  ggraph::geom_edge_link(ggplot2::aes(edge_alpha = n), show.legend = FALSE) +
  ggraph::geom_node_point(color = "lightblue", size = 5) +
  ggraph::geom_node_text(ggplot2::aes(label = name), vjust = 1, hjust = 1) +
  ggplot2::labs(title = "Bigram Word Network (TF ≥ 15)")
```


```{r ia phrase analysis}
# tokenize ia bigrams
ia_bigrams <- ia_book_texts_df %>%
  tidytext::unnest_tokens(bigram, text, token = "ngrams", n = 2)

# split and filter
ia_bigrams_filtered <- ia_bigrams %>%
  tidyr::separate(bigram, into = c("word1", "word2"), sep = " ") %>%
  dplyr::filter(!word1 %in% enhanced_stopwords, !word2 %in% enhanced_stopwords) %>%
  tidyr::unite(bigram, word1, word2, sep = " ")

# calculate ia tf
ia_bigram_tf <- ia_bigrams_filtered %>%
  dplyr::count(gutenberg_id, bigram, sort = TRUE)

# calculate ia tfidf
ia_bigram_tfidf <- ia_bigram_tf %>%
  tidytext::bind_tf_idf(term = bigram, document = gutenberg_id, n = n) %>%
  dplyr::left_join(
    fl_book_texts_df %>% dplyr::select(gutenberg_id, title),
    by = "gutenberg_id"
  )

# top bigrams per doc
top_ia_bigram_tfidf <- ia_bigram_tfidf %>%
  dplyr::group_by(gutenberg_id) %>%
  dplyr::slice_max(order_by = tf_idf, n = 10) %>%
  dplyr::ungroup()
```

```{r ia phrase analysis viz}
# split bigrams into related adjacent columns
ia_bigram_pairs <- ia_bigrams_filtered %>%
  tidyr::separate(bigram, into = c("word1", "word2"), sep = " ") %>%
  dplyr::count(word1, word2, sort = TRUE) %>%
  dplyr::filter(n >= 15)

ia_bigram_graph <- igraph::graph_from_data_frame(ia_bigram_pairs)

# plot bigram network
ggraph::ggraph(ia_bigram_graph, layout = "fr") +
  ggthemes::theme_wsj() +
  ggraph::geom_edge_link(ggplot2::aes(edge_alpha = n), show.legend = FALSE) +
  ggraph::geom_node_point(color = "lightblue", size = 5) +
  ggraph::geom_node_text(ggplot2::aes(label = name), vjust = 1, hjust = 1) +
  ggplot2::labs(title = "Bigram Word Network (TF ≥ 15)")
```

### Subgroup Comparisons

```{r shared vs unique terms}
# create a plot of shared versus unique terms
shared_terms <- generics::intersect(fl_tfidf$word, ia_tfidf$word)
fl_unique <- generics::setdiff(fl_tfidf$word, ia_tfidf$word)
ia_unique <- generics::setdiff(ia_tfidf$word, fl_tfidf$word)

summary_df <- tibble::tibble(
  Category = c("Shared", "Unique to Florida", "Unique to Iowa"),
  Count = c(length(shared_terms), length(fl_unique), length(ia_unique))
)

ggplot2::ggplot(summary_df, ggplot2::aes(x = Category, y = Count, fill = Category)) +
  ggthemes::theme_wsj() +
  ggplot2::geom_col(show.legend = FALSE) +
  ggplot2::labs(title = "Vocabulary Overlap Between Florida and Iowa Books")
```

```{r comparison cloud}
# use region label and count tf
fl_tf_region <- fl_tfidf %>%
  dplyr::select(word, n) %>%
  dplyr::mutate(region = "Florida")

ia_tf_region <- ia_tfidf %>%
  dplyr::select(word, n) %>%
  dplyr::mutate(region = "Iowa")

# combine and group
fl_ia_tf <- dplyr::bind_rows(fl_tf_region, ia_tf_region) %>%
  dplyr::group_by(region, word) %>%
  dplyr::summarise(freq = sum(n), .groups = "drop") %>%
  tidyr::pivot_wider(names_from = region, values_from = freq, values_fill = 0)

# set rownames
fl_ia_matrix <- fl_ia_tf %>%
  tibble::column_to_rownames("word") %>%
  as.matrix()

# plot
wordcloud::comparison.cloud(
  fl_ia_matrix,
  colors = c("blue", "darkgreen"),
  title.size = 1.2,
  max.words = 100,
  scale = c(3, 0.8),
  random.order = FALSE
)
```


### Dictionary-Based Analysis

```{r dictionary creation}
theme_dictionary <- list(
  gender = c("woman", "man", "trans", "nonbinary", "gender", "female", "male", "girls", "boys", "identity"),
  race = c("race", "racism", "black", "white", "asian", "latinx", "african american", "ethnicity", "heritage"),
  sexuality = c("lgbt", "queer", "gay", "lesbian", "bisexual", "homosexual", "transgender", "asexual", "sexuality"),
  violence = c("abuse", "violence", "murder", "kill", "assault", "trauma", "war", "fight", "weapon", "death"),
  politics = c("government", "democracy", "election", "law", "rights", "justice", "politics", "civil", "freedom")
)
```


```{r fl theme flags}
# standardize column type
fl_theme_flags <- fl_theme_flags %>%
  dplyr::mutate(text = as.character(text))

# use dictionary to compare themes
fl_theme_flags <- fl_book_texts_df %>%
  dplyr::mutate(
    across(
      .cols = tidyselect::everything(),
      .fns = ~ as.character(.)
    )
  ) %>%
  dplyr::mutate(
    dplyr::across(
      .cols = dplyr::everything(),
      .fns = ~ stringr::str_to_lower(.)
    )
  ) %>%
  dplyr::mutate(
    has_gender = purrr::map_lgl(description, ~ detect_theme(.x, theme_dictionary$gender)),
    has_race = purrr::map_lgl(description, ~ detect_theme(.x, theme_dictionary$race)),
    has_sexuality = purrr::map_lgl(description, ~ detect_theme(.x, theme_dictionary$sexuality)),
    has_violence = purrr::map_lgl(description, ~ detect_theme(.x, theme_dictionary$violence)),
    has_politics = purrr::map_lgl(description, ~ detect_theme(.x, theme_dictionary$politics))
  )
```

```{r ia theme flags}
# standardize column type
ia_theme_flags <- ia_theme_flags %>%
  dplyr::mutate(text = as.character(text))

# use dictionary to compare themes
ia_theme_flags <- ia_book_texts_df %>%
  dplyr::mutate(
    across(
      .cols = tidyselect::everything(),
      .fns = ~ as.character(.)
    )
  ) %>%
  dplyr::mutate(
    dplyr::across(
      .cols = dplyr::everything(),
      .fns = ~ stringr::str_to_lower(.)
    )
  ) %>%
  dplyr::mutate(
    has_gender = purrr::map_lgl(description, ~ detect_theme(.x, theme_dictionary$gender)),
    has_race = purrr::map_lgl(description, ~ detect_theme(.x, theme_dictionary$race)),
    has_sexuality = purrr::map_lgl(description, ~ detect_theme(.x, theme_dictionary$sexuality)),
    has_violence = purrr::map_lgl(description, ~ detect_theme(.x, theme_dictionary$violence)),
    has_politics = purrr::map_lgl(description, ~ detect_theme(.x, theme_dictionary$politics))
  )
```

```{r regional theme comparison}
# combine data sets
fl_ia_theme_counts <- dplyr::bind_rows(
  fl_theme_flags %>% dplyr::mutate(region = "Florida"),
  ia_theme_flags %>% dplyr::mutate(region = "Iowa")
)

# create a summary table
theme_summary <- fl_ia_theme_counts %>%
  tidyr::pivot_longer(cols = dplyr::starts_with("has_"), names_to = "theme", values_to = "present") %>%
  dplyr::group_by(region, theme) %>%
  dplyr::summarise(count = sum(present), .groups = "drop")

# plot
ggplot2::ggplot(theme_summary, ggplot2::aes(x = theme, y = count, fill = region)) +
  ggplot2::geom_col(position = "dodge") +
  ggplot2::labs(title = "Theme Prevalence in Book Descriptions by Region",
                x = "Theme", y = "Number of Books") +
  ggthemes::theme_wsj()
```

Need to finish this section (dictionary)

### Classification Model


### Sentiment Analysis


## Findings


## Conclusion

Need to save all visuals