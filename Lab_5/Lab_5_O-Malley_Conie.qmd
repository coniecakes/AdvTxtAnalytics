---
title: "Lab 5"
author: "Conie O'Malley"
date: "`r Sys.Date()`"
format: 
  pdf: 
    titlepage: true
    toc: true
    toc-depth: 2
    code-block-wrap: true
    number-sections: true
execute:
  echo: true
  freeze: false
  error: false
  jupyter: python3
python:
  version: /Users/coniecakes/anaconda3/envs/datascience/bin/python
---

```{r}
# install packages
packages <- c("quanteda", "quanteda.textmodels", "quanteda.textstats", "quanteda.textplots", "textdata","wordcloud", "readtext")


for (i in packages) {
    if (!requireNamespace(i, quietly = TRUE)) {
        renv::install(i) 
    }
    library(i, character.only = TRUE)  # Load the package
}

remotes::install_github("quanteda/quanteda.sentiment")
remotes::install_github("quanteda/quanteda.tidy")

renv::install("reshape2")

library(tm)
library(tidyverse)
library(tidytext)
library(reshape2)
library(janeaustenr)
library(reticulate)

#use_condaenv("datascience", required = FALSE) # set my environment
```

# Part 1: Data Preparation, Text Mining and Dictionary Development in tm

## Deliverable 1: Get your working directory and paste below:
```{r}
getwd()
```

## Deliverable 2: Create Files For Use from Reuters

```{r}
reut21578 <- system.file("texts","crude", package = "tm")
```

## Deliverable 3: Create VCorpus Object

```{r}
reuters <- VCorpus(DirSource(reut21578,mode = "binary"), 
                            readerControl = list(reader=readReut21578XMLasPlain))
reuters
```

## Deliverable 4: Prepare and Preprocess the Corpus

```{r}
reuters <- tm_map(reuters, content_transformer(tolower)) # make all text lowercase

reuters <- tm_map(reuters, removeWords, tm::stopwords("english")) # remove stopwords

myStopwords = c(tm::stopwords(),"") # alternate preprocessing method
tdm3 = TermDocumentMatrix(reuters,
                        control = list(weighting = weightTfIdf,
                        stopwords = myStopwords,
                        removePunctuation = T,
                        removeNumbers = T,
                        stemming = T))
inspect(tdm3)
```

## Deliverable 5: Create Document Term Matrix with TF and TF*IDF

```{r}
dtm <- DocumentTermMatrix(reuters) # create dtm
inspect(dtm)

dtm2 <- DocumentTermMatrix(reuters, control = list(weighting=weightTfIdf)) # dtm with idf weights
inspect(dtm2)
```

## Deliverable 6: Find the Most Frequent Terms

```{r}
findFreqTerms(dtm,5) # find all terms mentioned > 5 times
```

## Deliverable 7: Find Terms Associated with a Specific Term

```{r}
findAssocs(dtm, "opec", 0.8) # find terms associated with "opec"

findAssocs(dtm2, "opec", 0.8) # find terms associated with "opec"
```

#### Which do you find more useful?

The weighted version weeds out certain words that may not be critical to analysis, like "said", "oil", and "15.8". Presumably "oil" will be highly relational to opec (since its the first word of the opec acronym), said is a verb likely to come after the mention of opec, and 15.8 is a unknown float. The `TF*IDF` weighting method helps to reduce noise in the data like these terms above.

## Deliverable 8: Remove Sparse Terms

```{r}
inspect(removeSparseTerms(dtm, 0.4)) # remove sparse terms

inspect(removeSparseTerms(dtm2, 0.4)) # remove sparse terms
```

## Deliverable 9: Develop a Simple Dictionary in tm

```{r}
inspect(DocumentTermMatrix(reuters, list(dictionary = c("prices","crude","oil"))))
```

# Part 2: Understanding Tidyverse Dictionary Construction and Sentiment Analysis 

```{r}
sentiments
head(sentiments)
tail(sentiments)
class(sentiments)
```

## Deliverable 10: Download Individual Lexicons within Sentiments

```{r}
get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc")
```

## Deliverable 11: Create an object called tidy_books from the janeaustenr package

```{r}
tidy_books <- janeaustenr::austen_books() %>%
    group_by(book) %>% 
    mutate(linenumber = row_number(),
        chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", ignore_case = TRUE)))) %>% 
    ungroup() %>% 
    unnest_tokens(word, text)
tidy_books
```

## Deliverable 12: Create nrcjoy Sentiment Dictionary

```{r}
nrcjoy <- get_sentiments("nrc") %>%
    filter(sentiment == "joy")
nrcjoy
```

## Deliverable 13: Applying NRC Joy Extract to Emma

```{r}
tidy_books %>% 
    filter(book == "Emma") %>%
    inner_join(nrcjoy) %>%
    count(word, sort = TRUE)

tidy_books %>% 
    filter(book == "Persuasion") %>%
    inner_join(nrcjoy) %>%
    count(word, sort = TRUE)
```

#### This result is interesting, but how does the book Emma compare to other books by Jane Austen on the specific sentiment of joy?

After reviewing the sentiment analysis for the book *Persuasion*, we can see that 8/10 words from *Emma* are on the list. We can infer that these novels have a level of similarity in terms of the emotions they evoke, but also must account for the Austen's writing style to account for some of the similarities.

## Deliverable 14: Sentiment Analysis of Jane Austen Books

```{r}
janeaustensentiment <- tidy_books %>% 
    inner_join(get_sentiments("bing")) %>%
    count(book, index = linenumber %/% 80, sentiment) %>%
    spread(sentiment, n, fill = 0) %>% 
    mutate(sentiment = positive - negative)
```

## Deliverable 15: Visualize Jane Austen Sentiment

```{r}
ggplot(janeaustensentiment, aes(index, sentiment, fill = book)) + 
    geom_col(show.legend = FALSE) +
    facet_wrap(~book, ncol = 2, scales = "free_x") +
    labs(title = "Jane Austen Sentiment Analysis", x = "Index", y = "Sentiment")
```

## Deliverable 16: Calculate and Visualize Sentiment and Words

```{r}
bing_word_counts <- tidy_books %>% 
    inner_join(get_sentiments("bing")) %>% 
    count(word, sentiment, sort = TRUE) %>% 
    ungroup()
bing_word_counts


bing_word_counts %>%
    group_by(sentiment) %>%
    top_n(10) %>%
    ungroup() %>%
    mutate(word = reorder(word,n)) %>%
    ggplot(aes(word, n, fill = sentiment)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~sentiment, scales = "free_y") +
    labs(y = "Contribution to sentiment", x = NULL) +
    coord_flip()
```

## Deliverable 17: Create a Custom Stopword Dictionary

```{r}
custom_stop_words <- bind_rows(tibble(word = c("miss"), lexicon = c("custom")), stop_words)
custom_stop_words
```

## Deliverable 18: Apply Custom Stopword Dictionary

```{r}
bing_word_counts %>%
    anti_join(custom_stop_words) %>% 
    group_by(sentiment) %>% 
    top_n(10) %>% ungroup() %>%
    mutate(word = reorder(word, n)) %>%
    ggplot() +
    geom_col(aes(word, n, fill = sentiment), show.legend = F) +
    labs(title = "Sentiment Analysis of Jane Austen's Works",
        subtitle = "Separated by Sentiment",
        x = "",
        y = "Contribution to Sentiment") +
    theme_classic() +
    theme(plot.title = element_text(hjust = 0.5), 
        plot.subtitle = element_text(hjust = 0.5)) +
    scale_fill_brewer(palette = "Set1") +
    facet_wrap(~sentiment, scales = "free") +
    coord_flip()
```

## Deliverable 19: Data Visualization with WordClouds

```{r}
tidy_books %>%
    anti_join(stop_words) %>%
    count(word) %>%
    with(wordcloud(word, n, max.words = 100))

tidy_books %>%
    inner_join(get_sentiments("bing")) %>%
    count(word, sentiment, sort = TRUE) %>%
    acast(word ~ sentiment, value.var = "n", fill = 0) %>%
    comparison.cloud(colors = c("gray20","gray80"), max.words = 100)
```

# Part 3: Text Mining with quanteda, Including Variable Creation and Dictionaries

```{r}
global_path <- "/Users/coniecakes/Library/CloudStorage/OneDrive-Personal/001. Documents - Main/023. Programming Tools/R Studio/AdvTxtAnalytics/Lab 5/UN-data/"
```

## Deliverable 20: Create an Object for the UNGD Speeches

```{r}
UNGDspeeches <- readtext(paste0(global_path, "**/*.txt"),
                docvarsfrom = "filenames",
                docvarnames = c("country","session","year"),
                dvsep = "_",
                encoding = "UTF-8")
UNGDspeeches
class(UNGDspeeches)
```

## Deliverable 21: Generate a Corpus from UNGDspeeches

```{r}
mycorpus <- corpus(UNGDspeeches)

docvars(mycorpus, "Textno") <- sprintf("%02d", 1:ndoc(mycorpus))
mycorpus

mycorpus.stats <- summary(mycorpus)
head(mycorpus.stats, n=10)
```

## Deliverable 22: Preprocess the Text

```{r}
token <-tokens(mycorpus,
               split_hyphens = TRUE,
               remove_numbers = TRUE,
               remove_punct = TRUE,
               remove_symbols = TRUE,
               remove_url = TRUE,
               include_docvars = TRUE
)

token_ungd <- tokens_select(token, c("[\\d-]", "[[:punct:]]", "^.{1,2}$"),
                        selection = "remove",
                        valuetype = "regex",
                        verbose = TRUE
)
```

## Deliverable 23: Tokenize the Dataset by N-Grams

```{r}
toks_ngram <- tokens_ngrams(token, n = 2:4)
head(toks_ngram[[1]], 30)
tail(toks_ngram[[1]], 30)
```

## Deliverable 24: Create a Document Feature Matrix

```{r}
mydfm <- dfm(token_ungd, tolower = TRUE,)
mydfm <- dfm_remove(mydfm, pattern = stopwords("english"))
mydfm <- dfm_wordstem(mydfm)
```

## Deliverable 25: Trim the DFM

```{r}
mydfm.trim <- dfm_trim(mydfm, min_docfreq = 0.075,
                    max_docfreq = 0.90,
                    docfreq_type = "prop"
)

head(dfm_sort(mydfm.trim, decreasing = TRUE, margin = "both"), n = 10, nf = 10)
```

#### Which country refers most to the economy in this snapshot of the data?

Cuba.

## Deliverable 26: Text Classification Using a Dictionary

```{r}
dict <- dictionary(file = "/Users/coniecakes/Library/CloudStorage/OneDrive-Personal/001. Documents - Main/023. Programming Tools/R Studio/AdvTxtAnalytics/Lab 5/data/policy_agendas_english.lcd")
```

## Deliverable 27: Apply Dictionary

```{r}
mydfm.un <- dfm(mydfm.trim) # create DFM w/o grouping or applying dictionary
mydfm.un <- dfm_lookup(mydfm.un, dictionary = dict) # apply dictionary 
mydfm.un <- dfm_group(mydfm.un, groups = docvars(mydfm.un, "country")) # group the DFM by "country"
```

## Deliverable 28: Convert the DFM to a Data Frame

```{r}
un.topics.pa <- convert(mydfm.un, "data.frame") %>%
    dplyr::rename(country = doc_id) %>%
    select(country, immigration, intl_affairs, defence) %>%
    tidyr::gather(immigration:defence, key = "Topic", value = "Share") %>%
    group_by(country) %>%
    mutate(Share = Share/ sum(Share)) %>%
    mutate(Topic = haven::as_factor(Topic))
```

## Deliverable 29: Visualize the Results

```{r}
un.topics.pa %>%
    ggplot(aes(country, Share, colour = Topic, fill = Topic))+
    geom_bar(stat = "identity")+
    ggthemes::theme_economist_white() +
    scale_color_brewer(palette = "Set1")+
    scale_fill_brewer(palette = "Pastel1")+
    ggtitle("Distribution of PA topics in the UN General Debate corpus")+
    xlab("")+
    ylab("Topic share (%)")+
    theme(axis.text.x = element_blank(),
    axis.ticks.x = element_blank())
```

# Part 4: Using nltk and TextBlob to conduct sentiment analysis in Python

## Deliverable 30: Creating a Custom Lexicon and Applying it to a Sample Dataset

```{python}
custom_lexicon = {
'positive': ['good', 'great', 'awesome', 'fantastic', 'terrific'],
'negative': ['bad', 'terrible', 'awful', 'dreadful', 'horrible'],
'neutral': ['okay', 'alright', 'fine', 'decent', 'satisfactory'],
'uncertain': ['maybe', 'perhaps', 'possibly', 'probably', 'likely'],
'conjunctions': ['and', 'but', 'or', 'so', 'yet']
}
```

```{python}
import nltk
nltk.download('punkt')
nltk.download('punkt_tab')
```

```{python}
def preprocess_and_token(text):
    text = text.lower()
    tokens = text.split()
    return tokens
```
```{python}
def preprocess_and_tokenize(text):
    text = text.lower()
    tokens = text.split()
    return tokens
```

```{python}
def categorize_text(text, lexicon):
    tokens = preprocess_and_tokenize(text)
    categories = {category: 0 for category in lexicon}
    for token in tokens:
        for category, words in lexicon.items():
            if token in words:
                categories[category] += 1
    return categories
```

```{python}
sample_texts = [
    'The movie was good and the acting was great.',
    'The movie was terrible and the acting was dreadful.',
    'The movie was okay and the acting was satisfactory.',
    'The movie was perhaps good and the acting was probably great.',
    'The movie was fine and the acting was decent.',
    'The movie was good but the acting was terrible.',
    'The movie was good or the acting was bad.',
    'The movie was good so the acting was bad.',
    'The movie was good yet the acting was bad.'
]
for text in sample_texts:
    categorize = categorize_text(text, custom_lexicon)
    print(categorize_text(text, custom_lexicon))
```

## Deliverable 31: Adding N-Grams to the Custom Lexicon

```{python}
custom_lexicon = {
    'positive': ['good', 'great', 'awesome', 'fantastic', 'terrific', 'good and', 'great and'],
    'negative': ['bad', 'terrible', 'awful', 'dreadful', 'horrible', 'bad and', 'terrible and'],
    'neutral': ['okay', 'alright', 'fine', 'decent', 'satisfactory', 'okay and', 'alright and'],
    'uncertain': ['maybe', 'perhaps', 'possibly', 'probably', 'likely', 'maybe and', 'perhaps and'],
    'conjunctions': ['and', 'but', 'or', 'so', 'yet', 'but and', 'or and', 'so and', 'yet and']
}
```

## Deliverable 32: Applying the Custom Lexicon with N-Grams to the Sample Sentences


```{python}
from nltk.util import ngrams
```


```{python}
def generate_ngrams(tokens, n):
   return [' '.join(gram) for gram in ngrams(tokens, n)]
```


```{python 3}
def preprocess_and_tokenize(text):
    text = text.lower()
    tokens = text.split()
    bigrams = generate_ngrams(tokens, 2)
    trigrams = generate_ngrams(tokens, 3)
    all_tokens = tokens + bigrams + trigrams
    return all_tokens
```


```{python 4}
def categorize_text(text, lexicon):
    tokens = preprocess_and_tokenize(text)
    categories = {category: 0 for category in lexicon}
    for token in tokens:  
        for category, phrases in lexicon.items():
            if token in phrases:
                categories[category] += 1
    return categories
```


## Deliverable 33: Downloading NLTK Data and Preparing the Dataset

```{python}
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
nltk.download('vader_lexicon') # Download VADER lexicon
```

```{python}
# Initialize VADER sentiment analyzer
sia = SentimentIntensityAnalyzer()
# Sample text
text = "I love this product! It's absolutely amazing :)"
# Get sentiment scores
sentiment = sia.polarity_scores(text)
print(sentiment)
```

```{python}
import nltk
from nltk.corpus import movie_reviews
from nltk.sentiment import SentimentIntensityAnalyzer
import pandas as pd
nltk.download('movie_reviews')
nltk.download('vader_lexicon')
```

```{python}
documents = [(list(movie_reviews.words(fileid)), category)
            for category in movie_reviews.categories()
            for fileid in movie_reviews.fileids(category)]
```

```{python}
reviews = pd.DataFrame(documents, columns = ['text', 'sentiment'])
reviews['text'] = reviews['text'].apply(lambda x: ' '.join(x))
```

## Deliverable 34: Display the first Five Rows of the Reviews Dataframe
```{python}
print(reviews.head())
print(reviews.tail())
```

## Deliverable 35: Sentiment Analysis with VADER

```{python}
sid = SentimentIntensityAnalyzer()
reviews['scores'] = reviews['text'].apply(lambda review: sid.polarity_scores(review))
reviews['compound'] = reviews['scores'].apply(lambda score_dict: score_dict['compound'])
reviews['comp_score'] = reviews['compound'].apply(lambda c: 'pos' if c >=0 else 'neg')

print(reviews[['text', 'sentiment', 'compound', 'comp_score']].head())
```

## Deliverable 36: Quick Exploration of Sentiment Analysis in TextBlob

```{python}
import nltk
from textblob import TextBlob
nltk.download('gutenberg')
from nltk.corpus import gutenberg
```

```{python}
text = gutenberg.raw('austen-emma.txt') # import text

sentences = nltk.sent_tokenize(text) # split into sentences

for sentence in sentences[:25]:
    blob = TextBlob(sentence)
    print(f"Sentence: {sentence}\nPolarity: {blob.sentiment.polarity}\n")
```

## Deliverable 37: Sentiment Analysis on the UN Data with TextBlob

```{python}
import os
from textblob import TextBlob
```

```{python}
folder_path = "/Users/coniecakes/Library/CloudStorage/OneDrive-Personal/001. Documents - Main/023. Programming Tools/R Studio/AdvTxtAnalytics/Lab 5/UN-data"
results = [] # List to store the results
# Walk through all subdirectories and files
for root, dirs, files in os.walk(folder_path):
    for filename in files:
        if filename.endswith('.txt'):
            file_path = os.path.join(root, filename)
            with open(file_path, 'r', encoding='utf-8') as file:
                text = file.read()
                blob = TextBlob(text)
                polarity = blob.sentiment.polarity
                subjectivity = blob.sentiment.subjectivity
                results.append({
                    'file_path': file_path,
                    'polarity': polarity,
                    'subjectivity': subjectivity
                    })

for result in results[:5]:
    print(f"File: {result['file_path']}")
    print(f"Polarity: {result['polarity']}")
    print(f"Subjectivity: {result['subjectivity']}")
    print('---')
```