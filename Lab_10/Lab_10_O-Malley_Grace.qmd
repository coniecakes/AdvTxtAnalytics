---
title: "Lab 10"
author: "Grace O'Malley"
date: "`r Sys.Date()`"
format: 
  html:
    theme: cosmo
    toc: true
    toc-depth: 2
    code-fold: true
    number-sections: true
execute:
  echo: true
  freeze: false
  error: false
  jupyter: python3
---

```{r libraries}

required_packages <- c("remotes", "tidymodels", "discrim", "naivebayes", "quanteda.textmodels", 
                        "textrecipes", "workflows", "parsnip", "tune", "yardstick", "dials")

# Check if all required packages are installed and install them if not
for (pkg in required_packages) {
    if (!requireNamespace(pkg, quietly = TRUE)) {
        renv::install(pkg)
    }
        library(pkg, character.only = TRUE)
}

remotes::install_github("EmilHvitfeldt/scotus")
library(scotus)

# for reference to install later
python_packages <- c("pandas", "numpy", "matplotlib", "seaborn", "scikit-learn", "nltk")

```

## Deliverable 1: Get your working directory and paste below

*/Users/coniecakes/Library/CloudStorage/OneDrive-Personal/001. Documents - Main/023. Programming Tools/R Studio/AdvTxtAnalytics*

# Part 1: Regression Modeling

## Preparing the SCOTUS Data

```{r scotus data prep}

scotus_data <- scotus::scotus_filtered %>%
    tibble::as_tibble()

utils::head(scotus_data, 5)

glimpse(scotus_data)

scotus_data %>%
    dplyr::mutate(year = as.numeric(year), year = 10 * (year %/% 10)) %>%
    dplyr::count(year) %>%
    ggplot2::ggplot(ggplot2::aes(year, n))+
    ggplot2::geom_col()+
    ggplot2::labs(x="Year", y="Number of opinions per decade") +
    ggthemes::theme_economist_white()

```

## Building a Predictive Regression Model

```{r predictive regression model}

set_seed <- 1234

set.seed(set_seed)

scotus_split <- scotus::scotus_filtered %>%
    dplyr::mutate(year=as.numeric(year), text=stringr::str_remove_all(text,"'")) %>%
    rsample::initial_split()

scotus_train <- rsample::training(scotus_split)

scotus_test <- rsample::testing(scotus_split)

```

## Exploring Text Recipes and Workflows

```{r scotus data exploration}

scotus_rec <- recipes::recipe(year ~ text, data = scotus_train) %>%
    textrecipes::step_tokenize(text) %>%
    textrecipes::step_tokenfilter(text, max_tokens = 1e3) %>%
    textrecipes::step_tfidf(text) %>%
    recipes::step_normalize(recipes::all_predictors())

scotus_rec

```


```{r scotus bake}

scotus_prep <- recipes::prep(scotus_rec)
scotus_bake <- recipes::bake(scotus_prep, new_data=NULL)
dim(scotus_bake)

```

```{r scotus workflow}

scotus_wf <- workflows::workflow() %>%
    workflows::add_recipe(scotus_rec)

scotus_wf

```

```{r scotus svm model}

svm_spec <- parsnip::svm_linear() %>%
    parsnip::set_mode("regression") %>%
    parsnip::set_engine("LiblineaR")

svm_fit <- scotus_wf %>%
    workflows::add_model(svm_spec) %>%
    workflows::fit(data = scotus_train)

svm_fit %>%
    workflows::extract_fit_parsnip() %>%
    recipes::tidy() %>%
    dplyr::arrange(-estimate)

```


## Evaluating the Model

```{r test words a}

test_words1 <- svm_fit %>%
    workflows::extract_fit_parsnip() %>%
    recipes::tidy() %>%
    dplyr::slice_max(estimate, n = 10) %>%
    dplyr::mutate(term = stringr::str_remove(term, "tfidf_text_")) %>%
    dplyr::pull(term)

test_words1

```

```{r test words b}

svm_fit %>%
    workflows::extract_fit_parsnip() %>%
    recipes::tidy() %>%
    dplyr::arrange(estimate)

test_words2 <- svm_fit %>%
    workflows::extract_fit_parsnip() %>%
    recipes::tidy() %>%
    dplyr::slice_max(-estimate, n = 10) %>%
    dplyr::mutate(term = stringr::str_remove(term, "tfidf_text_")) %>%
    dplyr::pull(term)

test_words2

```

```{r scotus cv}

set_seed_2 <- 123

set.seed(set_seed_2)

scotus_folds <- rsample::vfold_cv(scotus_train)
scotus_folds

```

```{r scotus training data performance}

set.seed(set_seed_2)

svm_rs <- tune::fit_resamples(scotus_wf %>% 
    workflows::add_model(svm_spec), scotus_folds, control = control_resamples(save_pred = TRUE))

svm_rs

```

```{r scotus rmse}
tune::collect_metrics(svm_rs)

first_attemp_rmse <- tune::collect_metrics(svm_rs) %>%
    dplyr::filter(.metric == "rmse") %>%
    dplyr::pull(mean) %>%
    round(1)

svm_rs %>%
    tune::collect_predictions() %>%
    ggplot2::ggplot(ggplot2::aes(year, .pred, color = id)) +
    ggplot2::geom_abline(lty = 2, color = "gray80", size = 1.5) +
    ggplot2::geom_point(alpha = 0.3) +
    ggplot2::labs(
            x = "Truth",
            y = "Predicted year",
            color = NULL,
            title = "Predicted and true years for Supreme Court opinions",
            subtitle = "Each cross-validation fold is shown in a different color") +
    ggthemes::theme_economist_white()

```

```{r scotus null model}

null_regression <- parsnip::null_model() %>%
    parsnip::set_engine("parsnip") %>%
    parsnip::set_mode("regression")

null_rs <- tune::fit_resamples(scotus_wf %>% 
    workflows::add_model(null_regression), scotus_folds, metrics = yardstick::metric_set(rmse))

null_rs

tune::collect_metrics(null_rs)

```

# Part 2: Classification Modeling

## Preparing the Complaints Data

```{r complaints data prep}

complaints <- readr::read_csv("/Users/coniecakes/Library/CloudStorage/OneDrive-Personal/001. Documents - Main/023. Programming Tools/R Studio/AdvTxtAnalytics/data_files/data/complaints_sample_25k.csv")

glimpse(complaints)

utils::head(complaints$consumer_complaint_narrative)

utils::tail(complaints$consumer_complaint_narrative)

complaints$consumer_complaint_narrative %>%
    stringr::str_extract_all("\\{\\$[0-9\\.]*\\}") %>%
    purrr::compact() %>%
    utils::head()

```

## Creating a Two-Class Model and Splitting Data

```{r complaints two class model}

set.seed(set_seed)

complaints2class <- complaints %>%
    dplyr::mutate(
        product = stringr::str_trim(product),
        product = factor(dplyr::if_else(
                    product == paste("Credit reporting, credit repair services,",
                                    "or other personal consumer reports"),
                                    "Credit", "Other")))

complaints_split <- rsample::initial_split(complaints2class, strata = product)
complaints_train <- rsample::training(complaints_split)
complaints_test <- rsample::testing(complaints_split)

dim(complaints_train)
dim(complaints_test)

```

## Creating a Complaints Recipe and Workflow

```{r complaints recipe}

complaints_rec <-
    recipes::recipe(product ~ consumer_complaint_narrative, data = complaints_train)

complaints_rec <- complaints_rec %>%
    textrecipes::step_tokenize(consumer_complaint_narrative) %>%
    textrecipes::step_tokenfilter(consumer_complaint_narrative, max_tokens = 500) %>%
    textrecipes::step_tfidf(consumer_complaint_narrative)

```

```{r complaints workflow}

complaint_wf <- workflows::workflow() %>%
    workflows::add_recipe(complaints_rec)

nb_spec <- parsnip::naive_Bayes() %>%
    parsnip::set_mode("classification") %>%
    parsnip::set_engine("naivebayes")

nb_spec

nb_fit <- complaint_wf %>%
    workflows::add_model(nb_spec) %>%
    fit(data = complaints_train)

```

```{r complaints cv}

set_seed_3 <- 234

set.seed(set_seed_3)

complaints_folds <- rsample::vfold_cv(complaints_train, v = 5, strata = product)
complaints_folds

```

```{r complaints final workflow}

nb_wf <- workflows::workflow() %>%
    workflows::add_recipe(complaints_rec) %>%
    workflows::add_model(nb_spec)

nb_wf

```

## Evaluating the Naive Bayes Model

```{r nb model evaluation}

nb_rs <- tune::fit_resamples(nb_wf, complaints_folds, control = tune::control_resamples(save_pred = TRUE))

nb_rs_metrics <- tune::collect_metrics(nb_rs)
nb_rs_predictions <- tune::collect_predictions(nb_rs)
nb_rs_metrics

```

```{r nb evaluation viz a}

nb_rs_predictions %>%
    dplyr::group_by(id) %>%
    yardstick::roc_curve(truth = product, .pred_Credit) %>%
    autoplot() +
    ggplot2::labs(color = NULL,
                title = "ROC curve for US Consumer Finance Complaints",
                subtitle = "Each resample fold is shown in a different color") +
    ggthemes::theme_economist_white()

```

```{r nb evaluation viz b}
tune::conf_mat_resampled(nb_rs, tidy = FALSE) %>%
    autoplot(type = "heatmap")

```

```{r complaints null model}

null_classification <- parsnip::null_model() %>%
    parsnip::set_engine("parsnip") %>%
    parsnip::set_mode("classification")

null_rs <- workflows::workflow() %>%
    workflows::add_recipe(complaints_rec) %>%
    workflows::add_model(null_classification) %>%
    tune::fit_resamples(complaints_folds)

null_rs %>%
    tune::collect_metrics()

```

## Exploring a Lasso Model

```{r lasso model}

lasso_spec <- parsnip::logistic_reg(penalty = 0.01, mixture = 1) %>%
    parsnip::set_mode("classification") %>%
    parsnip::set_engine("glmnet")

lasso_spec

lasso_wf <- workflows::workflow() %>%
    workflows::add_recipe(complaints_rec) %>%
    workflows::add_model(lasso_spec)

lasso_wf

```

```{r lasso model fit}

set_seed_4 <- 2020
set.seed(set_seed_4)

lasso_rs <- tune::fit_resamples(lasso_wf, complaints_folds, control = tune::control_resamples(save_pred = TRUE))

lasso_rs_metrics <- tune::collect_metrics(lasso_rs)
lasso_rs_predictions <- tune::collect_predictions(lasso_rs)
lasso_rs_metrics

```

```{r lasso model viz a}

lasso_rs_predictions %>%
    dplyr::group_by(id) %>%
    yardstick::roc_curve(truth = product, .pred_Credit) %>%
    autoplot() +
    ggplot2::labs(color = NULL, 
                title = "ROC curve for US Consumer Finance Complaints", 
                subtitle = "Each resample fold is shown in a different color") +
    ggthemes::theme_economist_white()

```

```{r lasso model viz b}
tune::conf_mat_resampled(lasso_rs, tidy = FALSE) %>%
    autoplot(type = "heatmap")

```

## Tuning Model Hyperparameters

```{r hyperparameter tuning}

tune_spec <- logistic_reg(penalty = tune(), mixture = 1) %>%
    parsnip::set_mode("classification") %>%
    parsnip::set_engine("glmnet")
tune_spec

lambda_grid <- dials::grid_regular(dials::penalty(), levels = 15)
lambda_grid

```

```{r tuned workflow}

tune_wf <- workflows::workflow() %>%
    workflows::add_recipe(complaints_rec) %>%
    workflows::add_model(tune_spec)

set.seed(set_seed_4)

tune_rs <- tune::tune_grid(tune_wf, complaints_folds, grid = lambda_grid, 
                            control = tune::control_resamples(save_pred = TRUE))

tune_rs

```


## Evaluating the Lasso Model

```{r lasso model evaluation}

tune::collect_metrics(tune_rs)

autoplot(tune_rs) +
    ggplot2::labs(
                title = "Lasso model performance across regularization penalties",
                subtitle = "Performance metrics can be used to identity the best penalty") +
    ggthemes::theme_economist_white()

```

```{r final lasso tuning}

tune_rs %>% tune::collect_metrics() %>%
    dplyr::filter(.metric == "roc_auc") %>%
    dplyr::arrange(desc(mean)) %>%
    dplyr::slice(1) %>%
    dplyr::pull(mean) %>%
    round(3)

chosen_auc <- tune_rs %>%
    tune::select_by_one_std_err(metric = "roc_auc", -penalty)
chosen_auc

final_lasso <- tune::finalize_workflow(tune_wf, chosen_auc)
final_lasso

```

```{r final lasso model}

fitted_lasso <- fit(final_lasso, complaints_train)

fitted_lasso %>%
    workflows::extract_fit_parsnip() %>%
    recipes::tidy() %>%
    dplyr::arrange(-estimate)

fitted_lasso %>%
    workflows::extract_fit_parsnip() %>%
    recipes::tidy() %>%
    dplyr::arrange(estimate)

```