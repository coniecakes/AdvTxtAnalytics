---
title: "Lab 10"
author: "Grace O'Malley"
date: "`r Sys.Date()`"
format: 
  html:
    theme: cosmo
    toc: true
    toc-depth: 2
    code-fold: true
    number-sections: true
execute:
  echo: true
  freeze: false
  error: false
  jupyter: python3
---

```{r libraries}

required_packages <- c("remotes", "tidymodels", "discrim", "naivebayes", "quanteda.textmodels", 
                        "textrecipes", "workflows", "parsnip", "tune", "yardstick", "dials")

# Check if all required packages are installed and install them if not
for (pkg in required_packages) {
    if (!requireNamespace(pkg, quietly = TRUE)) {
        renv::install(pkg)
    }
        library(pkg, character.only = TRUE)
}

remotes::install_github("EmilHvitfeldt/scotus")
library(scotus)

# for reference to install later
python_packages <- c("pandas", "numpy", "matplotlib", "seaborn", "scikit-learn", "nltk")

```

## Deliverable 1: Get your working directory and paste below

*/Users/coniecakes/Library/CloudStorage/OneDrive-Personal/001. Documents - Main/023. Programming Tools/R Studio/AdvTxtAnalytics*

# Part 1: Regression Modeling

## Preparing the SCOTUS Data

```{r scotus data prep}

scotus_data <- scotus::scotus_filtered %>%
    tibble::as_tibble()

utils::head(scotus_data, 5)

glimpse(scotus_data)

scotus_data %>%
    dplyr::mutate(year = as.numeric(year), year = 10 * (year %/% 10)) %>%
    dplyr::count(year) %>%
    ggplot2::ggplot(ggplot2::aes(year, n))+
    ggplot2::geom_col()+
    ggplot2::labs(x="Year", y="Number of opinions per decade") +
    ggthemes::theme_economist_white()

```

## Building a Predictive Regression Model

```{r predictive regression model}

set_seed <- 1234

set.seed(set_seed)

scotus_split <- scotus::scotus_filtered %>%
    dplyr::mutate(year=as.numeric(year), text=stringr::str_remove_all(text,"'")) %>%
    rsample::initial_split()

scotus_train <- rsample::training(scotus_split)

scotus_test <- rsample::testing(scotus_split)

```

## Exploring Text Recipes and Workflows

```{r scotus data exploration}

scotus_rec <- recipes::recipe(year ~ text, data = scotus_train) %>%
    textrecipes::step_tokenize(text) %>%
    textrecipes::step_tokenfilter(text, max_tokens = 1e3) %>%
    textrecipes::step_tfidf(text) %>%
    recipes::step_normalize(recipes::all_predictors())

scotus_rec

```


```{r scotus bake}

scotus_prep <- recipes::prep(scotus_rec)
scotus_bake <- recipes::bake(scotus_prep, new_data=NULL)
dim(scotus_bake)

```

```{r scotus workflow}

scotus_wf <- workflows::workflow() %>%
    workflows::add_recipe(scotus_rec)

scotus_wf

```

```{r scotus svm model}

svm_spec <- parsnip::svm_linear() %>%
    parsnip::set_mode("regression") %>%
    parsnip::set_engine("LiblineaR")

svm_fit <- scotus_wf %>%
    workflows::add_model(svm_spec) %>%
    workflows::fit(data = scotus_train)

svm_fit %>%
    workflows::extract_fit_parsnip() %>%
    recipes::tidy() %>%
    dplyr::arrange(-estimate)

```


## Evaluating the Model

```{r test words a}

test_words1 <- svm_fit %>%
    workflows::extract_fit_parsnip() %>%
    recipes::tidy() %>%
    dplyr::slice_max(estimate, n = 10) %>%
    dplyr::mutate(term = stringr::str_remove(term, "tfidf_text_")) %>%
    dplyr::pull(term)

test_words1

```

```{r test words b}

svm_fit %>%
    workflows::extract_fit_parsnip() %>%
    recipes::tidy() %>%
    dplyr::arrange(estimate)

test_words2 <- svm_fit %>%
    workflows::extract_fit_parsnip() %>%
    recipes::tidy() %>%
    dplyr::slice_max(-estimate, n = 10) %>%
    dplyr::mutate(term = stringr::str_remove(term, "tfidf_text_")) %>%
    dplyr::pull(term)

test_words2

```

```{r scotus cv}

set_seed_2 <- 123

set.seed(set_seed_2)

scotus_folds <- rsample::vfold_cv(scotus_train)
scotus_folds

```

```{r scotus training data performance}

set.seed(set_seed_2)

svm_rs <- tune::fit_resamples(scotus_wf %>% 
    workflows::add_model(svm_spec), scotus_folds, control = control_resamples(save_pred = TRUE))

svm_rs

```

```{r scotus rmse}
tune::collect_metrics(svm_rs)

first_attemp_rmse <- tune::collect_metrics(svm_rs) %>%
    dplyr::filter(.metric == "rmse") %>%
    dplyr::pull(mean) %>%
    round(1)

svm_rs %>%
    tune::collect_predictions() %>%
    ggplot2::ggplot(ggplot2::aes(year, .pred, color = id)) +
    ggplot2::geom_abline(lty = 2, color = "gray80", size = 1.5) +
    ggplot2::geom_point(alpha = 0.3) +
    ggplot2::labs(
            x = "Truth",
            y = "Predicted year",
            color = NULL,
            title = "Predicted and true years for Supreme Court opinions",
            subtitle = "Each cross-validation fold is shown in a different color") +
    ggthemes::theme_economist_white()

```

```{r scotus null model}

null_regression <- parsnip::null_model() %>%
    parsnip::set_engine("parsnip") %>%
    parsnip::set_mode("regression")

null_rs <- tune::fit_resamples( scotus_wf %>% 
    workflows::add_model(null_regression), scotus_folds, metrics = yardstick::metric_set(rmse))

null_rs

tune::collect_metrics(null_rs)

```

# Part 2: Classification Modeling

## Preparing the Complaints Data

```{r complaints data prep}

complaints <- readr::read_csv("/Users/coniecakes/Library/CloudStorage/OneDrive-Personal/001. Documents - Main/023. Programming Tools/R Studio/AdvTxtAnalytics/data_files/data/complaints_sample_25k.csv")

glimpse(complaints)

utils::head(complaints$consumer_complaint_narrative)

utils::tail(complaints$consumer_complaint_narrative)

complaints$consumer_complaint_narrative %>%
    stringr::str_extract_all("\\{\\$[0-9\\.]*\\}") %>%
    purrr::compact() %>%
    utils::head()

```

## Creating a Two-Class Model and Splitting Data

```{r complaints two class model}

set.seed(set_seed)

complaints2class <- complaints %>%
    dplyr::mutate(product = factor(dplyr::if_else(
                    product == paste0("Credit reporting, credit repair services,",
                                    "or other personal consumer reports"),
                                    "Credit", "Other")))

complaints_split <- rsample::initial_split(complaints2class, strata = product)
complaints_train <- rsample::training(complaints_split)
complaints_test <- rsample::testing(complaints_split)

dim(complaints_train)
dim(complaints_test)

```

## Creating a Complaints Recipe and Workflow

```{r complaints recipe}

complaints_rec <-
    recipes::recipe(product ~ consumer_complaint_narrative, data = complaints_train)

complaints_rec <- complaints_rec %>%
    textrecipes::step_tokenize(consumer_complaint_narrative) %>%
    textrecipes::step_tokenfilter(consumer_complaint_narrative, max_tokens = 500) %>%
    textrecipes::step_tfidf(consumer_complaint_narrative)

```

```{r complaints workflow}

complaint_wf <- workflows::workflow() %>%
    workflows::add_recipe(complaints_rec)

nb_spec <- parsnip::naive_Bayes() %>%
    parsnip::set_mode("classification") %>%
    parsnip::set_engine("naivebayes")

nb_spec

nb_fit <- complaint_wf %>%
    workflows::add_model(nb_spec) %>%
    fit(data = complaints_train)

```

```{r complaints cv}

set.seed(234)
complaints_folds <- vfold_cv(complaints_train, v = 5)
complaints_folds

```