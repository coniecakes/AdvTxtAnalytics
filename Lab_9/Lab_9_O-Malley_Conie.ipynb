{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 9 - Python Code\n",
    "### Conie O'Malley\n",
    "\n",
    "\n",
    "## Part 6: Clustering in Python: K-Means, DBSCAN, and Hierarchical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Importing Libraries and Loading Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from wordcloud import WordCloud\n",
    "import umap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python data load\n",
    "categories = ['comp.graphics', 'rec.sport.baseball', 'sci.space', 'talk.politics.guns']\n",
    "newsgroups = fetch_20newsgroups(categories=categories, remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating TF-IDF Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer(max_features=5000,\n",
    "stop_words='english',\n",
    "max_df=0.95,\n",
    "min_df=2)\n",
    "tfidf_matrix = vectorizer.fit_transform(newsgroups.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TruncatedSVD (LSA) first\n",
    "n_components = 100\n",
    "lsa = TruncatedSVD(n_components=n_components)\n",
    "doc_vectors_lsa = lsa.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Further reduce to 2D using UMAP for visualization\n",
    "reducer = umap.UMAP(random_state=42)\n",
    "doc_vectors_2d = reducer.fit_transform(doc_vectors_lsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing for and Conducting K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_kmeans_clustering(vectors, n_clusters=4):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(vectors)\n",
    "    # Get cluster centers\n",
    "    if vectors.shape[1] > 2:\n",
    "        centers_2d = reducer.transform(kmeans.cluster_centers_)\n",
    "    else:\n",
    "        centers_2d = kmeans.cluster_centers_\n",
    "    return kmeans, cluster_labels, centers_2d\n",
    "\n",
    "# Perform k-means clustering\n",
    "kmeans_model, kmeans_labels, centers = perform_kmeans_clustering(doc_vectors_lsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize K-means Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k means cluster viz}\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(doc_vectors_2d[:, 0], doc_vectors_2d[:, 1],\n",
    "c=kmeans_labels, cmap='viridis', alpha=0.6)\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', marker='x', s=200, linewidths=3)\n",
    "plt.title('K-means Clustering of News Articles')\n",
    "plt.colorbar(scatter)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_hierarchical_clustering(vectors, n_clusters=4):\n",
    "    # Perform hierarchical clustering\n",
    "    hc = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "    hc_labels = hc.fit_predict(vectors)\n",
    "    # Create linkage matrix for dendrogram\n",
    "    linkage_matrix = linkage(vectors, method='ward')\n",
    "    return hc, hc_labels, linkage_matrix\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "hc_model, hc_labels, linkage_matrix = perform_hierarchical_clustering(doc_vectors_lsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "dendrogram(linkage_matrix)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_dbscan_clustering(vectors, eps=0.5, min_samples=5):\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    dbscan_labels = dbscan.fit_predict(vectors)\n",
    "    return dbscan, dbscan_labels\n",
    "\n",
    "# Perform DBSCAN clustering\n",
    "dbscan_model, dbscan_labels = perform_dbscan_clustering(doc_vectors_lsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Analysis and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_clusters(vectors, labels, vectorizer):\n",
    "    # Convert sparse matrix to dense array\n",
    "    vectors_dense = vectors.toarray() if hasattr(vectors, 'todense') else vectors\n",
    "\n",
    "    # Calculate silhouette score\n",
    "    sil_score = silhouette_score(vectors_dense, labels)\n",
    "    # Calculate Calinski-Harabasz score\n",
    "    ch_score = calinski_harabasz_score(vectors_dense, labels)\n",
    "    \n",
    "    # Get top terms per cluster\n",
    "    n_terms = 10\n",
    "    cluster_terms = {}\n",
    "    # Original feature names\n",
    "    feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "    # For each cluster\n",
    "    for cluster_id in np.unique(labels):\n",
    "        if cluster_id != -1:  # Ignore noise points from DBSCAN\n",
    "            # Get points in cluster\n",
    "            cluster_docs = vectors[labels == cluster_id].toarray() if hasattr(vectors, 'todense') else vectors[labels == cluster_id]\n",
    "            # Calculate mean tfidf scores for each term\n",
    "            centroid = cluster_docs.mean(axis=0)\n",
    "            # Get top terms\n",
    "            top_term_indices = centroid.argsort()[-n_terms:][::-1]\n",
    "            top_terms = feature_names[top_term_indices]\n",
    "            cluster_terms[f'Cluster {cluster_id}'] = top_terms\n",
    "\n",
    "    return {\n",
    "        'silhouette_score': sil_score,\n",
    "        'calinski_harabasz_score': ch_score,\n",
    "        'top_terms': cluster_terms\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "kmeans_analysis = analyze_clusters(tfidf_matrix, kmeans_labels, vectorizer)\n",
    "hc_analysis = analyze_clusters(tfidf_matrix, hc_labels, vectorizer)\n",
    "dbscan_analysis = analyze_clusters(tfidf_matrix, dbscan_labels, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow Method for K-means\n",
    "def plot_elbow_curve(vectors, max_k=10):\n",
    "    distortions = []\n",
    "    K = range(1, max_k+1)\n",
    "    for k in K:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(vectors)\n",
    "        distortions.append(kmeans.inertia_)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(K, distortions, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Distortion')\n",
    "    plt.title('The Elbow Method for Optimal k')\n",
    "    plt.show()\n",
    "\n",
    "plot_elbow_curve(doc_vectors_lsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Word Clouds for Clusters\n",
    "def generate_cluster_wordcloud(vectors, labels, vectorizer, cluster_id):\n",
    "    # Get documents in cluster\n",
    "    cluster_docs = vectors[labels == cluster_id]\n",
    "    # Get feature names\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    # Calculate mean tfidf scores\n",
    "    centroid = cluster_docs.mean(axis=0).A1 if hasattr(cluster_docs, 'todense') else cluster_docs.mean(axis=0)\n",
    "    # Create word frequency dictionary\n",
    "    word_freq = {feature_names[i]: centroid[i]\n",
    "                 for i in range(len(feature_names))\n",
    "                 if centroid[i] > 0}\n",
    "\n",
    "    # Generate word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400,\n",
    "                          background_color='white').generate_from_frequencies(word_freq)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Word Cloud for Cluster {cluster_id}')\n",
    "    plt.show()\n",
    "\n",
    "# Generate word clouds for K-means clusters\n",
    "for cluster_id in np.unique(kmeans_labels):\n",
    "    generate_cluster_wordcloud(tfidf_matrix, kmeans_labels, vectorizer, cluster_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Analysis Visualization\n",
    "def plot_silhouette_analysis(vectors, labels):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    silhouette_avg = silhouette_score(vectors, labels)\n",
    "    sample_silhouette_values = silhouette_samples(vectors, labels)\n",
    "    y_lower = 10\n",
    "    for i in range(len(np.unique(labels))):\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        plt.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                            0, ith_cluster_silhouette_values,\n",
    "                            alpha=0.7)\n",
    "        y_lower = y_upper + 10\n",
    "\n",
    "    plt.title(\"Silhouette Analysis\")\n",
    "    plt.xlabel(\"Silhouette Coefficient\")\n",
    "    plt.ylabel(\"Cluster\")\n",
    "    plt.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "    plt.show()\n",
    "\n",
    "# Plot silhouette analysis for K-means\n",
    "from sklearn.metrics import silhouette_samples\n",
    "\n",
    "plot_silhouette_analysis(doc_vectors_lsa, kmeans_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with Topic Modeling\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "def compare_clustering_with_lda(vectors, cluster_labels, n_topics=4):\n",
    "    # Perform LDA\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    doc_topics = lda.fit_transform(vectors)\n",
    "    topic_labels = doc_topics.argmax(axis=1)\n",
    "\n",
    "    # Create confusion matrix\n",
    "    confusion_matrix = pd.crosstab(cluster_labels, topic_labels,\n",
    "                                    normalize='index')\n",
    "\n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(confusion_matrix, annot=True, cmap='YlOrRd')\n",
    "    plt.title('Cluster vs Topic Distribution')\n",
    "    plt.xlabel('Topic')\n",
    "    plt.ylabel('Cluster')\n",
    "    plt.show()\n",
    "\n",
    "# Compare K-means clusters with LDA topics\n",
    "compare_clustering_with_lda(tfidf_matrix, kmeans_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: LDA Topic Modelling in Python\n",
    "\n",
    "### Importing Libraries and Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import nltk\n",
    "from gensim import corpora, models\n",
    "import matplotlib.pyplot as plt\n",
    "# Download NLTK data (run this only once)\n",
    "nltk.download('reuters')\n",
    "\n",
    "# Load the Reuters dataset from NLTK\n",
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of documents for simplicity (e.g., first 100 documents)\n",
    "docs = [reuters.raw(fileid) for fileid in reuters.fileids()[:100]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess(text):\n",
    "    # Convert text to lowercase, tokenize, remove punctuation and stopwords\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "    return tokens\n",
    "\n",
    "# Preprocess each document\n",
    "processed_docs = [preprocess(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Dictionary and Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the documents\n",
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "\n",
    "# Filter out extremes to remove noise and infrequent words\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=100000)\n",
    "\n",
    "# Convert the dictionary to a bag-of-words corpus\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying LDA Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LDA model\n",
    "lda_model = models.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=10)\n",
    "\n",
    "# Print the topics\n",
    "#for index, topic in lda_model.print_topics():\n",
    "#    print(f\"Topic: {index} \\nWords: {topic}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "# Prepare visualization of the topics\n",
    "vis = gensimvis.prepare(lda_model, corpus, dictionary)\n",
    "\n",
    "# Save the visualization to an HTML file\n",
    "pyLDAvis.save_html(vis, 'lda_visualization.html')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
