---
title: "Lab 5"
author: "Conie O'Malley"
date: "`r Sys.Date()`"
format: 
  pdf: 
    titlepage: true
    toc: true
    toc-depth: 2
    code-block-wrap: true
    number-sections: true
execute:
  echo: true
  freeze: false
  error: false
editor: source
python:
  version: /Users/coniecakes/anaconda3/envs/datascience/bin/python
---

```{r}
# install packages
packages <- c("quanteda", "quanteda.textmodels", "quanteda.textstats", "quanteda.textplots", "textdata","wordcloud", "readtext")


for (i in packages) {
    if (!requireNamespace(i, quietly = TRUE)) {
        renv::install(i) 
    }
    library(i, character.only = TRUE)  # Load the package
}

remotes::install_github("quanteda/quanteda.sentiment")
remotes::install_github("quanteda/quanteda.tidy")

renv::install("reshape2")

library(tm)
library(tidyverse)
library(tidytext)
library(reshape2)
library(janeaustenr)
```

# Part 1: Data Preparation, Text Mining and Dictionary Development in tm

## Deliverable 1: Get your working directory and paste below:
```{r}
getwd()
```

## Deliverable 2: Create Files For Use from Reuters

```{r}
reut21578 <- system.file("texts","crude", package = "tm")
```

## Deliverable 3: Create VCorpus Object

```{r}
reuters <- VCorpus(DirSource(reut21578,mode = "binary"), 
                            readerControl = list(reader=readReut21578XMLasPlain))
reuters
```

## Deliverable 4: Prepare and Preprocess the Corpus

```{r}
reuters <- tm_map(reuters, content_transformer(tolower)) # make all text lowercase

reuters <- tm_map(reuters, removeWords, tm::stopwords("english")) # remove stopwords

myStopwords = c(tm::stopwords(),"") # alternate preprocessing method
tdm3 = TermDocumentMatrix(reuters,
                        control = list(weighting = weightTfIdf,
                        stopwords = myStopwords,
                        removePunctuation = T,
                        removeNumbers = T,
                        stemming = T))
inspect(tdm3)
```

## Deliverable 5: Create Document Term Matrix with TF and TF*IDF

```{r}
dtm <- DocumentTermMatrix(reuters) # create dtm
inspect(dtm)

dtm2 <- DocumentTermMatrix(reuters, control = list(weighting=weightTfIdf)) # dtm with idf weights
inspect(dtm2)
```

## Deliverable 6: Find the Most Frequent Terms

```{r}
findFreqTerms(dtm,5) # find all terms mentioned > 5 times
```

## Deliverable 7: Find Terms Associated with a Specific Term

```{r}
findAssocs(dtm, "opec", 0.8) # find terms associated with "opec"

findAssocs(dtm2, "opec", 0.8) # find terms associated with "opec"
```

#### Which do you find more useful?

The weighted version weeds out certain words that may not be critical to analysis, like "said", "oil", and "15.8". Presumably "oil" will be highly relational to opec (since its the first word of the opec acronym), said is a verb likely to come after the mention of opec, and 15.8 is a unknown float. The `TF*IDF` weighting method helps to reduce noise in the data like these terms above.

## Deliverable 8: Remove Sparse Terms

```{r}
inspect(removeSparseTerms(dtm, 0.4)) # remove sparse terms

inspect(removeSparseTerms(dtm2, 0.4)) # remove sparse terms
```

## Deliverable 9: Develop a Simple Dictionary in tm

```{r}
inspect(DocumentTermMatrix(reuters, list(dictionary = c("prices","crude","oil"))))
```

# Part 2: Understanding Tidyverse Dictionary Construction and Sentiment Analysis 

```{r}
sentiments
head(sentiments)
tail(sentiments)
class(sentiments)
```

## Deliverable 10: Download Individual Lexicons within Sentiments

```{r}
get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc")
```

## Deliverable 11: Create an object called tidy_books from the janeaustenr package

```{r}
tidy_books <- janeaustenr::austen_books() %>%
    group_by(book) %>% 
    mutate(linenumber = row_number(),
        chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", ignore_case = TRUE)))) %>% 
    ungroup() %>% 
    unnest_tokens(word, text)
tidy_books
```

## Deliverable 12: Create nrcjoy Sentiment Dictionary

```{r}
nrcjoy <- get_sentiments("nrc") %>%
    filter(sentiment == "joy")
nrcjoy
```

## Deliverable 13: Applying NRC Joy Extract to Emma

```{r}
tidy_books %>% 
    filter(book == "Emma") %>%
    inner_join(nrcjoy) %>%
    count(word, sort = TRUE)

tidy_books %>% 
    filter(book == "Persuasion") %>%
    inner_join(nrcjoy) %>%
    count(word, sort = TRUE)
```

#### This result is interesting, but how does the book Emma compare to other books by Jane Austen on the specific sentiment of joy?

After reviewing the sentiment analysis for the book *Persuasion*, we can see that 8/10 words from *Emma* are on the list. We can infer that these novels have a level of similarity in terms of the emotions they evoke, but also must account for the Austen's writing style to account for some of the similarities.

## Deliverable 14: Sentiment Analysis of Jane Austen Books

```{r}
janeaustensentiment <- tidy_books %>% 
    inner_join(get_sentiments("bing")) %>%
    count(book, index = linenumber %/% 80, sentiment) %>%
    spread(sentiment, n, fill = 0) %>% 
    mutate(sentiment = positive - negative)
```

## Deliverable 15: Visualize Jane Austen Sentiment

```{r}
ggplot(janeaustensentiment, aes(index, sentiment, fill = book)) + 
    geom_col(show.legend = FALSE) +
    facet_wrap(~book, ncol = 2, scales = "free_x") +
    labs(title = "Jane Austen Sentiment Analysis", x = "Index", y = "Sentiment")
```

## Deliverable 16: Calculate and Visualize Sentiment and Words

```{r}
bing_word_counts <- tidy_books %>% 
    inner_join(get_sentiments("bing")) %>% 
    count(word, sentiment, sort = TRUE) %>% 
    ungroup()
bing_word_counts


bing_word_counts %>%
    group_by(sentiment) %>%
    top_n(10) %>%
    ungroup() %>%
    mutate(word = reorder(word,n)) %>%
    ggplot(aes(word, n, fill = sentiment)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~sentiment, scales = "free_y") +
    labs(y = "Contribution to sentiment", x = NULL) +
    coord_flip()
```

## Deliverable 17: Create a Custom Stopword Dictionary

```{r}
custom_stop_words <- bind_rows(tibble(word = c("miss"), lexicon = c("custom")), stop_words)
custom_stop_words
```

## Deliverable 18: Apply Custom Stopword Dictionary

```{r}
bing_word_counts %>%
    anti_join(custom_stop_words) %>% 
    group_by(sentiment) %>% 
    top_n(10) %>% ungroup() %>%
    mutate(word = reorder(word, n)) %>%
    ggplot() +
    geom_col(aes(word, n, fill = sentiment), show.legend = F) +
    labs(title = "Sentiment Analysis of Jane Austen's Works",
        subtitle = "Separated by Sentiment",
        x = "",
        y = "Contribution to Sentiment") +
    theme_classic() +
    theme(plot.title = element_text(hjust = 0.5), 
        plot.subtitle = element_text(hjust = 0.5)) +
    scale_fill_brewer(palette = "Set1") +
    facet_wrap(~sentiment, scales = "free") +
    coord_flip()
```

## Deliverable 19: Data Visualization with WordClouds

```{r}
tidy_books %>%
    anti_join(stop_words) %>%
    count(word) %>%
    with(wordcloud(word, n, max.words = 100))

tidy_books %>%
    inner_join(get_sentiments("bing")) %>%
    count(word, sentiment, sort = TRUE) %>%
    acast(word ~ sentiment, value.var = "n", fill = 0) %>%
    comparison.cloud(colors = c("gray20","gray80"), max.words = 100)
```

# Part 3: Text Mining with quanteda, Including Variable Creation and Dictionaries

```{r}
global_path <- "/Users/coniecakes/Library/CloudStorage/OneDrive-Personal/001. Documents - Main/023. Programming Tools/R Studio/AdvTxtAnalytics/Lab 5/UN-data/"
```

## Deliverable 20: Create an Object for the UNGD Speeches

```{r}
UNGDspeeches <- readtext(paste0(global_path, "**/*.txt"),
                docvarsfrom = "filenames",
                docvarnames = c("country","session","year"),
                dvsep = "_",
                encoding = "UTF-8")
UNGDspeeches
class(UNGDspeeches)
```

## Deliverable 21: Generate a Corpus from UNGDspeeches

```{r}
mycorpus <- corpus(UNGDspeeches)

docvars(mycorpus, "Textno") <- sprintf("%02d", 1:ndoc(mycorpus))
mycorpus

mycorpus.stats <- summary(mycorpus)
head(mycorpus.stats, n=10)
```

## Deliverable 22: Preprocess the Text

```{r}
token <-tokens(mycorpus,
               split_hyphens = TRUE,
               remove_numbers = TRUE,
               remove_punct = TRUE,
               remove_symbols = TRUE,
               remove_url = TRUE,
               include_docvars = TRUE
)

token_ungd <- tokens_select(token, c("[\\d-]", "[[:punct:]]", "^.{1,2}$"),
                        selection = "remove",
                        valuetype = "regex",
                        verbose = TRUE
)
```

## Deliverable 23: Tokenize the Dataset by N-Grams



## Deliverable 24: Create a Document Feature Matrix